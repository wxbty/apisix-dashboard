import*as arrays from"../../../base/common/arrays.js";import{onUnexpectedError}from"../../../base/common/errors.js";import{LineTokens}from"../tokens/lineTokens.js";import{TokenizationRegistry}from"../languages.js";import{nullTokenizeEncoded}from"../languages/nullMode.js";import{Disposable}from"../../../base/common/lifecycle.js";import{StopWatch}from"../../../base/common/stopwatch.js";import{countEOL}from"../core/eolCounter.js";import{ContiguousMultilineTokensBuilder}from"../tokens/contiguousMultilineTokensBuilder.js";import{runWhenIdle}from"../../../base/common/async.js";import{setTimeout0}from"../../../base/common/platform.js";class ContiguousGrowingArray{constructor(t){this._default=t,this._store=[]}get(t){return t<this._store.length?this._store[t]:this._default}set(t,e){while(t>=this._store.length)this._store[this._store.length]=this._default;this._store[t]=e}delete(t,e){0===e||t>=this._store.length||this._store.splice(t,e)}insert(t,e){if(0===e||t>=this._store.length)return;const i=[];for(let n=0;n<e;n++)i[n]=this._default;this._store=arrays.arrayInsert(this._store,t,i)}}export class TokenizationStateStore{constructor(t,e){this.tokenizationSupport=t,this.initialState=e,this._lineBeginState=new ContiguousGrowingArray(null),this._lineNeedsTokenization=new ContiguousGrowingArray(!0),this._firstLineNeedsTokenization=0,this._lineBeginState.set(0,this.initialState)}get invalidLineStartIndex(){return this._firstLineNeedsTokenization}markMustBeTokenized(t){this._lineNeedsTokenization.set(t,!0),this._firstLineNeedsTokenization=Math.min(this._firstLineNeedsTokenization,t)}getBeginState(t){return this._lineBeginState.get(t)}setEndState(t,e,i){if(this._lineNeedsTokenization.set(e,!1),this._firstLineNeedsTokenization=e+1,e===t-1)return;const n=this._lineBeginState.get(e+1);if(null===n||!i.equals(n))return this._lineBeginState.set(e+1,i),void this.markMustBeTokenized(e+1);let o=e+1;while(o<t){if(this._lineNeedsTokenization.get(o))break;o++}this._firstLineNeedsTokenization=o}applyEdits(t,e){this.markMustBeTokenized(t.startLineNumber-1),this._lineBeginState.delete(t.startLineNumber,t.endLineNumber-t.startLineNumber),this._lineNeedsTokenization.delete(t.startLineNumber,t.endLineNumber-t.startLineNumber),this._lineBeginState.insert(t.startLineNumber,e),this._lineNeedsTokenization.insert(t.startLineNumber,e)}}export class TextModelTokenization extends Disposable{constructor(t,e){super(),this._textModel=t,this._languageIdCodec=e,this._isScheduled=!1,this._isDisposed=!1,this._tokenizationStateStore=null,this._register(TokenizationRegistry.onDidChange((t=>{const e=this._textModel.getLanguageId();-1!==t.changedLanguages.indexOf(e)&&(this._resetTokenizationState(),this._textModel.clearTokens())}))),this._register(this._textModel.onDidChangeContentFast((t=>{if(t.isFlush)this._resetTokenizationState();else{if(this._tokenizationStateStore)for(let e=0,i=t.changes.length;e<i;e++){const i=t.changes[e],[n]=countEOL(i.text);this._tokenizationStateStore.applyEdits(i.range,n)}this._beginBackgroundTokenization()}}))),this._register(this._textModel.onDidChangeAttached((()=>{this._beginBackgroundTokenization()}))),this._register(this._textModel.onDidChangeLanguage((()=>{this._resetTokenizationState(),this._textModel.clearTokens()}))),this._resetTokenizationState()}dispose(){this._isDisposed=!0,super.dispose()}_resetTokenizationState(){const[t,e]=initializeTokenization(this._textModel);this._tokenizationStateStore=t&&e?new TokenizationStateStore(t,e):null,this._beginBackgroundTokenization()}_beginBackgroundTokenization(){!this._isScheduled&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._isScheduled=!0,runWhenIdle((t=>{this._isScheduled=!1,this._backgroundTokenizeWithDeadline(t)})))}_backgroundTokenizeWithDeadline(t){const e=Date.now()+t.timeRemaining(),i=()=>{!this._isDisposed&&this._textModel.isAttachedToEditor()&&this._hasLinesToTokenize()&&(this._backgroundTokenizeForAtLeast1ms(),Date.now()<e?setTimeout0(i):this._beginBackgroundTokenization())};i()}_backgroundTokenizeForAtLeast1ms(){const t=this._textModel.getLineCount(),e=new ContiguousMultilineTokensBuilder,i=StopWatch.create(!1);do{if(i.elapsed()>1)break;const n=this._tokenizeOneInvalidLine(e);if(n>=t)break}while(this._hasLinesToTokenize());this._textModel.setTokens(e.finalize(),!this._hasLinesToTokenize())}tokenizeViewport(t,e){const i=new ContiguousMultilineTokensBuilder;this._tokenizeViewport(i,t,e),this._textModel.setTokens(i.finalize(),!this._hasLinesToTokenize())}reset(){this._resetTokenizationState(),this._textModel.clearTokens()}forceTokenization(t){const e=new ContiguousMultilineTokensBuilder;this._updateTokensUntilLine(e,t),this._textModel.setTokens(e.finalize(),!this._hasLinesToTokenize())}getTokenTypeIfInsertingCharacter(t,e){if(!this._tokenizationStateStore)return 0;this.forceTokenization(t.lineNumber);const i=this._tokenizationStateStore.getBeginState(t.lineNumber-1);if(!i)return 0;const n=this._textModel.getLanguageId(),o=this._textModel.getLineContent(t.lineNumber),s=o.substring(0,t.column-1)+e+o.substring(t.column-1),a=safeTokenize(this._languageIdCodec,n,this._tokenizationStateStore.tokenizationSupport,s,!0,i),r=new LineTokens(a.tokens,s,this._languageIdCodec);if(0===r.getCount())return 0;const h=r.findTokenIndexAtOffset(t.column-1);return r.getStandardTokenType(h)}tokenizeLineWithEdit(t,e,i){const n=t.lineNumber,o=t.column;if(!this._tokenizationStateStore)return null;this.forceTokenization(n);const s=this._tokenizationStateStore.getBeginState(n-1);if(!s)return null;const a=this._textModel.getLineContent(n),r=a.substring(0,o-1)+i+a.substring(o-1+e),h=this._textModel.getLanguageIdAtPosition(n,0),l=safeTokenize(this._languageIdCodec,h,this._tokenizationStateStore.tokenizationSupport,r,!0,s),d=new LineTokens(l.tokens,r,this._languageIdCodec);return d}isCheapToTokenize(t){if(!this._tokenizationStateStore)return!0;const e=this._tokenizationStateStore.invalidLineStartIndex+1;return!(t>e)&&(t<e||this._textModel.getLineLength(t)<2048)}_hasLinesToTokenize(){return!!this._tokenizationStateStore&&this._tokenizationStateStore.invalidLineStartIndex<this._textModel.getLineCount()}_tokenizeOneInvalidLine(t){if(!this._tokenizationStateStore||!this._hasLinesToTokenize())return this._textModel.getLineCount()+1;const e=this._tokenizationStateStore.invalidLineStartIndex+1;return this._updateTokensUntilLine(t,e),e}_updateTokensUntilLine(t,e){if(!this._tokenizationStateStore)return;const i=this._textModel.getLanguageId(),n=this._textModel.getLineCount(),o=e-1;for(let s=this._tokenizationStateStore.invalidLineStartIndex;s<=o;s++){const e=this._textModel.getLineContent(s+1),o=this._tokenizationStateStore.getBeginState(s),a=safeTokenize(this._languageIdCodec,i,this._tokenizationStateStore.tokenizationSupport,e,!0,o);t.add(s+1,a.tokens),this._tokenizationStateStore.setEndState(n,s,a.endState),s=this._tokenizationStateStore.invalidLineStartIndex-1}}_tokenizeViewport(t,e,i){if(!this._tokenizationStateStore)return;if(i<=this._tokenizationStateStore.invalidLineStartIndex)return;if(e<=this._tokenizationStateStore.invalidLineStartIndex)return void this._updateTokensUntilLine(t,i);let n=this._textModel.getLineFirstNonWhitespaceColumn(e);const o=[];let s=null;for(let h=e-1;n>1&&h>=1;h--){const t=this._textModel.getLineFirstNonWhitespaceColumn(h);if(0!==t&&t<n){if(s=this._tokenizationStateStore.getBeginState(h-1),s)break;o.push(this._textModel.getLineContent(h)),n=t}}s||(s=this._tokenizationStateStore.initialState);const a=this._textModel.getLanguageId();let r=s;for(let h=o.length-1;h>=0;h--){const t=safeTokenize(this._languageIdCodec,a,this._tokenizationStateStore.tokenizationSupport,o[h],!1,r);r=t.endState}for(let h=e;h<=i;h++){const e=this._textModel.getLineContent(h),i=safeTokenize(this._languageIdCodec,a,this._tokenizationStateStore.tokenizationSupport,e,!0,r);t.add(h,i.tokens),this._tokenizationStateStore.markMustBeTokenized(h-1),r=i.endState}}}function initializeTokenization(t){if(t.isTooLargeForTokenization())return[null,null];const e=TokenizationRegistry.get(t.getLanguageId());if(!e)return[null,null];let i;try{i=e.getInitialState()}catch(n){return onUnexpectedError(n),[null,null]}return[e,i]}function safeTokenize(t,e,i,n,o,s){let a=null;if(i)try{a=i.tokenizeEncoded(n,o,s.clone())}catch(r){onUnexpectedError(r)}return a||(a=nullTokenizeEncoded(t.encodeLanguageId(e),s)),LineTokens.convertToEndOffset(a.tokens,n.length),a}